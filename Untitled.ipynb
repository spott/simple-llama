{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f57f0a-61e7-43a8-96fe-20a65be702a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import basicConfig, INFO, DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8571cc76-74a0-45c8-ad6d-9342f685434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basicConfig(level=INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bab1c2-a564-4b0a-be7a-8210fa5ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e7b8f4-1628-4dd9-9af1-db4bb5f311d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reloaded SentencePiece model from ./models/llama-2-tokenizer/tokenizer.model\n",
      "INFO:root:#words: 32000 - BOS ID: 1 - EOS ID: 2\n",
      "INFO:__name__:model_args: {'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': 32000, 'max_seq_len': 100, 'max_batch_size': 1, 'ffn_dim_multiplier': None}\n",
      "INFO:__name__:state_dict_map: ['tok_embeddings.weight', 'norm.weight', 'output.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wq.weight', 'layers.28.attention.wk.weight', 'layers.28.attention.wv.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.attention_norm.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wq.weight', 'layers.29.attention.wk.weight', 'layers.29.attention.wv.weight', 'layers.29.attention.wo.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.attention_norm.weight', 'layers.29.ffn_norm.weight', 'layers.30.attention.wq.weight', 'layers.30.attention.wk.weight', 'layers.30.attention.wv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.attention_norm.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.31.attention.wk.weight', 'layers.31.attention.wv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.attention_norm.weight', 'layers.31.ffn_norm.weight', 'rope.freqs']\n",
      "INFO:__name__:unexpected_keys: ['rope.freqs']\n",
      "INFO:__name__:missing_keys: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded in 58.74 seconds\n"
     ]
    }
   ],
   "source": [
    "l = Llama.build(\"./models/llama-2-7B/\", \"./models/llama-2-tokenizer/tokenizer.model\", 100, 1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3253b2-4344-4546-9780-aebef5cd70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871,    -1,    -1,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 7, 32000]), tensor([[-3.2920, -3.3906,  4.5845,  7.2137,  7.0010,  6.0668,  7.8408,  4.7773,\n",
      "          4.5006,  5.9170,  6.2270,  7.9149,  5.0486,  7.8506,  1.2543,  1.0769,\n",
      "         -3.2924,  2.3283,  2.9091,  6.4330,  2.7863,  4.0528,  5.6706,  5.2769,\n",
      "          3.0353,  2.6701,  2.8476,  6.1066,  4.2034,  4.2319,  6.5470,  1.7805,\n",
      "          1.9513,  0.5828,  2.1561, -3.2926, -3.2927, -3.2917, -3.2913, -3.2907,\n",
      "         -3.2923, -3.2919, -3.2927, -3.2932, -3.2922, -3.2917, -3.2922, -3.2928,\n",
      "         -3.2936, -3.2924, -3.2915, -3.2922, -3.2913, -3.2921, -3.2925, -3.2920,\n",
      "         -3.2930, -3.2921, -3.2927, -3.2936, -3.2914, -3.2928, -3.2920, -3.2912,\n",
      "         -3.2921, -3.2918, -3.2909, -3.2929, -3.2913, -3.2920, -3.2917, -3.2916,\n",
      "         -3.2917, -3.2925, -3.2933, -3.2918, -3.2918, -3.2916, -3.2921, -3.2915,\n",
      "         -3.2916, -3.2928, -3.2926, -3.2915, -3.2918, -3.2904, -3.2928, -3.2918,\n",
      "         -3.2918, -3.2928, -3.2915, -3.2913, -3.2914, -3.2919, -3.2915, -3.2930,\n",
      "         -3.2924, -3.2916, -3.2922, -3.2933, -3.2919, -3.2928, -3.2911, -3.2907,\n",
      "         -3.2925, -3.2918, -3.2931, -3.2916, -3.2929, -3.2928, -3.2926, -3.2934,\n",
      "         -3.2917, -3.2928, -3.2921, -3.2910, -3.2919, -3.2922, -3.2927, -3.2915,\n",
      "         -3.2915, -3.2917, -3.2925, -3.2909, -3.2920, -3.2919, -3.2933, -3.2929,\n",
      "         -3.2915, -3.2920,  8.6922,  2.6106, -1.2283,  1.8671,  0.1700,  2.0673,\n",
      "          2.0927,  0.7809,  0.9830,  0.7851,  1.2749,  0.1893,  3.4148,  0.4483,\n",
      "          1.3580,  0.6412, -0.6685,  0.3422,  1.3989,  2.1787,  0.2910,  1.4462,\n",
      "          1.6543,  0.8778, -0.7947,  0.6672,  1.3678,  1.2514,  0.3731,  0.1813,\n",
      "          0.9495, -0.4495,  1.7539,  2.2706,  0.4421, -0.0282, -0.4387,  2.2796,\n",
      "         -0.3120,  1.0754, -1.2149,  1.2700,  1.7147,  2.2175,  1.5454, -0.2675,\n",
      "          0.2312,  0.9878,  2.2326,  1.0593,  2.0261,  1.1826, -0.9913,  3.3483,\n",
      "          0.9914, -0.9277,  1.5084, -0.1265,  2.7312,  1.2059,  0.7267,  1.3986,\n",
      "          1.6120,  0.5218,  1.3051, -3.2905, -3.2925, 10.7725, -3.2927,  8.2242,\n",
      "          6.6127,  7.6695,  9.0476,  7.4611,  6.8466,  8.0411,  8.9271,  8.5194,\n",
      "          7.1397,  6.3016,  7.6017,  3.9567,  8.4642,  7.4678,  6.7174,  8.7578,\n",
      "          7.0494,  7.5127,  5.5175,  5.6549,  7.1544,  3.3416,  7.2849,  3.3426,\n",
      "          2.2979,  0.9721,  5.4929,  7.6915, 10.1866, 11.0693,  6.4229,  7.4795,\n",
      "          7.8395,  7.6821,  7.3239,  7.3813,  7.1108,  6.1938,  6.6982,  7.4269,\n",
      "          6.6911,  6.8526,  9.4968, 11.6168, -3.1049, -4.4010,  2.2500,  2.0657,\n",
      "         -3.2923, -3.2916, -3.2921, -3.2913, -3.2920, -3.2923, -3.2918, -3.2918,\n",
      "         -3.2916, -3.2927, -3.2921, -0.1119,  3.9016,  1.5502,  0.2392,  7.2390,\n",
      "         -1.9930, -0.8554,  3.2820,  0.2269,  0.0646,  3.3206,  2.4608,  0.0652,\n",
      "          0.4038, -0.5710,  1.3025, -2.1788,  2.2146,  0.0736,  6.6364, -2.2638,\n",
      "         -2.4770,  4.0495,  4.4987,  0.0662,  0.0246,  2.3087,  2.3811,  0.5636,\n",
      "          1.4748,  2.7464, -1.1466,  0.7041, -0.2407,  0.6931, -0.6286, -0.8141,\n",
      "          1.9051,  6.1524,  2.2853,  8.9825]])\n",
      "INFO:__name__:next_token: [29896]\n",
      "INFO:__name__:next_token: 1\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[ 0.0000, -3.1171, -7.8360, -1.0087, -5.7597, -2.9218, -7.4001, -1.4114,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 29896,    -1,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-3.9884, -1.8592,  9.5427,  3.6980,  2.3115,  1.4738,  2.0470, -0.7992,\n",
      "         -0.1286,  1.6244,  0.3820,  1.9450,  6.9284, 13.1248, -1.0656, -2.2861,\n",
      "         -3.9901, -1.6965, -1.0672,  0.5320, -0.5977, -1.6290, -0.8197,  0.4664,\n",
      "         -0.3471, -1.7595, -2.1713,  2.7859, -0.6732, -0.0499,  0.2661, -1.6947,\n",
      "          0.1266, -0.2190,  0.5088, -3.9893, -3.9877, -3.9890, -3.9889, -3.9884,\n",
      "         -3.9888, -3.9896, -3.9896, -3.9883, -3.9896, -3.9884, -3.9888, -3.9898,\n",
      "         -3.9897, -3.9893, -3.9893, -3.9888, -3.9885, -3.9902, -3.9896, -3.9887,\n",
      "         -3.9896, -3.9895, -3.9895, -3.9895, -3.9892, -3.9894, -3.9907, -3.9885,\n",
      "         -3.9881, -3.9889, -3.9884, -3.9900, -3.9884, -3.9894, -3.9890, -3.9892,\n",
      "         -3.9894, -3.9898, -3.9902, -3.9889, -3.9885, -3.9874, -3.9879, -3.9892,\n",
      "         -3.9882, -3.9891, -3.9895, -3.9874, -3.9887, -3.9893, -3.9890, -3.9894,\n",
      "         -3.9898, -3.9894, -3.9896, -3.9880, -3.9888, -3.9880, -3.9895, -3.9887,\n",
      "         -3.9896, -3.9887, -3.9885, -3.9892, -3.9901, -3.9895, -3.9895, -3.9886,\n",
      "         -3.9895, -3.9886, -3.9891, -3.9889, -3.9894, -3.9887, -3.9891, -3.9895,\n",
      "         -3.9890, -3.9891, -3.9897, -3.9883, -3.9892, -3.9883, -3.9890, -3.9885,\n",
      "         -3.9884, -3.9888, -3.9900, -3.9884, -3.9891, -3.9891, -3.9899, -3.9891,\n",
      "         -3.9884, -3.9890,  1.1235,  2.5391, -0.3012,  1.8756, -0.1833,  0.1939,\n",
      "          2.9381,  0.7936,  2.5599,  2.5838,  1.0617,  1.2447,  1.6261, -0.4046,\n",
      "          1.6599,  0.2513,  1.0517,  0.6242,  0.1167,  2.4372, -0.0918, -0.2352,\n",
      "          0.8376,  1.0051,  0.2234,  2.3825,  2.9686,  2.5487,  0.1704,  2.7152,\n",
      "         -1.0303,  1.1639,  1.4748,  0.3189, -2.0318,  0.7589,  1.4613,  3.2539,\n",
      "          0.6595,  0.2454,  0.8907,  1.0683, -0.1630, -0.6276,  1.2288, -3.1639,\n",
      "          0.9176, -1.5380,  1.3957, -0.4124,  0.9514,  1.9832, -0.8950,  2.4911,\n",
      "          2.0538,  0.2018,  1.6782,  2.6281,  2.6533,  2.6592,  1.8102,  2.0066,\n",
      "         -0.5282, -0.8971, -0.0363, -3.9887, -3.9884,  4.7575, -3.9890,  2.3795,\n",
      "          0.4915,  2.9470,  3.2259,  3.9755,  0.8052,  4.5574,  5.2010,  5.7261,\n",
      "          1.9979,  3.8996,  1.7652,  2.1430,  2.8133, -0.6751, -0.1134,  0.5252,\n",
      "          1.9683,  3.2059, -0.4609,  2.9438,  5.2952, -1.9764,  4.7356, -0.6898,\n",
      "         -0.5629, -1.8724, -0.4070,  3.1533,  5.6155,  8.1233,  3.6762,  4.0930,\n",
      "          3.3204,  3.0972,  3.6427,  2.9781,  3.5073,  2.7439,  2.8350,  3.0965,\n",
      "          1.8184,  0.2651,  4.5946,  5.4055, -1.7439, -2.0221, -2.6144, -1.6058,\n",
      "         -3.9891, -3.9899, -3.9897, -3.9888, -3.9886, -3.9881, -3.9894, -3.9892,\n",
      "         -3.9887, -3.9888, -3.9889,  4.8619,  8.5269,  7.9020,  8.9081,  9.9791,\n",
      "          5.8560,  7.5530,  7.8850,  4.4158,  3.7633,  8.8504,  8.3793,  5.4800,\n",
      "          5.7469,  6.0784,  7.9904,  5.9201,  5.3612,  3.7295,  8.6469,  4.5819,\n",
      "          5.9108,  7.7907, 10.8133,  4.4041,  5.4258,  8.1967, 10.3783,  5.4387,\n",
      "          9.9451,  8.7041,  3.4530,  2.7582,  5.8767,  4.6369,  5.6325,  2.4090,\n",
      "          3.5866, 12.6574,  9.0310,  8.6784]])\n",
      "INFO:__name__:next_token: [29900]\n",
      "INFO:__name__:next_token: 0\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[ 0.0000, -3.1171, -7.8360, -1.0087, -5.7597, -2.9218, -7.4001, -1.4114,\n",
      "         -0.7412,  0.0000,  0.0000,  0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 29896, 29900,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-8.2389, -5.0031,  6.9660, -1.0728, -1.0295, -1.8511, -2.8927, -5.5789,\n",
      "         -3.5521, -3.5816, -4.0278, -1.1246,  2.5982, 10.0569, -4.5870, -4.3324,\n",
      "         -8.2412, -5.8827, -6.0840, -4.1040, -5.2061, -4.5063, -4.4823, -4.7572,\n",
      "         -5.4341, -6.2091, -6.4439, -1.7327, -3.7739, -3.2816, -4.5957, -5.3846,\n",
      "         -4.5709, -4.0305, -2.9270, -8.2411, -8.2384, -8.2399, -8.2403, -8.2402,\n",
      "         -8.2391, -8.2412, -8.2410, -8.2405, -8.2407, -8.2394, -8.2399, -8.2408,\n",
      "         -8.2402, -8.2401, -8.2397, -8.2412, -8.2399, -8.2407, -8.2410, -8.2400,\n",
      "         -8.2405, -8.2410, -8.2403, -8.2414, -8.2401, -8.2402, -8.2420, -8.2391,\n",
      "         -8.2395, -8.2405, -8.2409, -8.2409, -8.2394, -8.2405, -8.2403, -8.2413,\n",
      "         -8.2397, -8.2401, -8.2410, -8.2409, -8.2397, -8.2399, -8.2386, -8.2402,\n",
      "         -8.2393, -8.2398, -8.2416, -8.2375, -8.2403, -8.2402, -8.2404, -8.2421,\n",
      "         -8.2416, -8.2405, -8.2398, -8.2403, -8.2401, -8.2392, -8.2405, -8.2402,\n",
      "         -8.2405, -8.2397, -8.2394, -8.2398, -8.2426, -8.2408, -8.2416, -8.2401,\n",
      "         -8.2401, -8.2405, -8.2410, -8.2410, -8.2412, -8.2401, -8.2409, -8.2415,\n",
      "         -8.2402, -8.2410, -8.2409, -8.2416, -8.2407, -8.2402, -8.2393, -8.2403,\n",
      "         -8.2400, -8.2397, -8.2409, -8.2408, -8.2409, -8.2407, -8.2401, -8.2406,\n",
      "         -8.2395, -8.2417, -3.9146, -0.8685, -4.0937, -1.9849, -4.5831, -1.8595,\n",
      "         -3.4334, -4.0359, -0.8465, -2.7074, -2.9356, -5.0301, -2.2680, -5.6107,\n",
      "         -4.6568, -3.4224, -4.0531, -3.8626, -2.4727, -2.7632, -5.1280, -5.2821,\n",
      "         -1.7039, -2.0080, -6.0062, -0.3409, -2.3188, -3.2725, -3.9256, -2.4355,\n",
      "         -7.0799, -4.8965, -2.4354, -4.8244, -4.9665, -3.6084, -1.8996, -2.4668,\n",
      "         -4.6729, -1.3427, -3.9217, -3.9582, -3.8072, -5.1646, -3.7816, -5.2523,\n",
      "         -4.3187, -2.5892, -4.4361, -4.2002, -3.0752, -3.8108, -3.6886, -1.8756,\n",
      "         -0.7466, -4.2879, -2.5158, -2.2470, -2.2255, -1.7281, -3.9021, -1.4607,\n",
      "         -4.1372, -5.5531, -4.8943, -8.2402, -8.2407,  1.0055, -8.2400, -4.4026,\n",
      "         -3.2688, -0.0834, -0.4519, -2.4939, -3.4704,  0.7577,  0.1094, -0.8438,\n",
      "         -2.0071, -2.5736, -2.3017, -1.7141, -1.6153, -5.4491, -5.3690, -3.4273,\n",
      "         -2.5825, -0.4309, -5.3556, -2.0599, -0.6375, -5.0236, -0.6528, -4.8187,\n",
      "         -5.8794, -6.3552, -4.1726, -0.1757,  0.9976,  3.8953, -0.1760, -0.9490,\n",
      "          0.1277, -0.5326, -0.3664, -1.2454, -0.9169, -0.7539, -1.8034, -0.6485,\n",
      "         -1.7251, -2.2248,  1.5822,  2.4303, -7.7907, -7.4671, -4.9114, -5.5448,\n",
      "         -8.2410, -8.2421, -8.2418, -8.2394, -8.2395, -8.2407, -8.2401, -8.2403,\n",
      "         -8.2403, -8.2415, -8.2401,  1.4201,  6.1265,  2.7833,  4.8364,  7.8367,\n",
      "          0.0956,  2.1153,  6.3520,  1.7990, -0.0744,  5.5652,  5.3119,  2.2353,\n",
      "          1.2498,  1.3359,  6.2861,  2.0604,  0.2580,  0.8127,  5.9449,  0.6504,\n",
      "          0.9687,  6.3210,  8.6014,  1.0800,  1.6727,  5.6488,  6.6829,  0.2999,\n",
      "          6.9765,  6.0574,  0.6359, -1.5123,  3.1313, -0.4178,  1.2640, -0.0735,\n",
      "         -0.4575,  8.9074,  6.2303,  2.8879]])\n",
      "INFO:__name__:next_token: [29900]\n",
      "INFO:__name__:next_token: 0\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[ 0.0000, -3.1171, -7.8360, -1.0087, -5.7597, -2.9218, -7.4001, -1.4114,\n",
      "         -0.7412, -0.3361,  0.0000,  0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 29896, 29900, 29900,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-8.3012, -4.7160,  7.5002, -0.0523,  0.0389, -1.1720, -0.3848, -4.0838,\n",
      "         -2.0218, -1.3638, -1.6949,  1.0604,  4.6501, 10.7857, -3.1106, -2.1961,\n",
      "         -8.3019, -4.5565, -4.3278, -3.2163, -3.3529, -2.9907, -4.5663, -3.6632,\n",
      "         -4.7132, -5.2690, -4.3569, -2.3616, -2.6510, -2.7997, -2.8125, -4.1521,\n",
      "         -2.9315, -2.7846, -1.8848, -8.3024, -8.2997, -8.3005, -8.3019, -8.3007,\n",
      "         -8.2996, -8.3020, -8.3015, -8.3019, -8.3021, -8.3007, -8.3002, -8.3019,\n",
      "         -8.3015, -8.3012, -8.3006, -8.3018, -8.3000, -8.3016, -8.3018, -8.3012,\n",
      "         -8.3010, -8.3022, -8.3016, -8.3025, -8.3011, -8.3010, -8.3024, -8.3000,\n",
      "         -8.3014, -8.3021, -8.3017, -8.3016, -8.3011, -8.3015, -8.3015, -8.3018,\n",
      "         -8.3016, -8.3010, -8.3022, -8.3018, -8.3006, -8.3018, -8.2998, -8.3012,\n",
      "         -8.3008, -8.3017, -8.3022, -8.2997, -8.3015, -8.3009, -8.3007, -8.3025,\n",
      "         -8.3028, -8.3016, -8.3010, -8.3016, -8.3010, -8.3005, -8.3013, -8.3021,\n",
      "         -8.3016, -8.3008, -8.3012, -8.3006, -8.3033, -8.3015, -8.3024, -8.3013,\n",
      "         -8.3015, -8.3010, -8.3020, -8.3021, -8.3017, -8.3006, -8.3018, -8.3015,\n",
      "         -8.3014, -8.3018, -8.3019, -8.3023, -8.3020, -8.3014, -8.3007, -8.3010,\n",
      "         -8.3010, -8.3017, -8.3018, -8.3015, -8.3014, -8.3018, -8.3009, -8.3014,\n",
      "         -8.3004, -8.3021, -2.7873, -1.4956, -3.2954,  0.4019, -4.4240, -1.1110,\n",
      "         -2.4635, -1.4756,  0.0640, -1.2148, -2.2253, -3.8374, -1.0451, -4.7551,\n",
      "         -2.5604, -1.3550, -2.1967, -2.2329, -2.5060, -3.9640, -3.8581, -3.8496,\n",
      "         -0.8937, -0.9895, -3.1626,  0.5381, -1.3516, -2.7621, -2.8158, -0.3532,\n",
      "         -2.8605, -3.0256, -1.6449, -2.7382, -4.5685, -2.7752, -1.8724, -1.5386,\n",
      "         -3.3397, -1.3358, -4.0833, -1.0839, -2.7467, -2.9911, -4.2125, -4.2213,\n",
      "         -2.9236, -1.0899, -0.9461, -2.1641, -2.4428, -2.1091, -4.2760, -1.0008,\n",
      "          0.9857, -3.7351, -2.3465, -3.1120, -1.9015, -1.7586, -2.6448, -2.7819,\n",
      "         -3.3427, -3.1267, -3.0382, -8.3010, -8.3017,  1.8695, -8.3017, -1.6972,\n",
      "         -2.1264, -1.4476, -0.4017, -0.4766, -2.8057,  0.4266,  1.3376,  0.7751,\n",
      "         -1.2050, -1.7689, -2.1926, -1.5893, -1.8608, -3.5288, -3.5100, -2.5670,\n",
      "         -1.8018, -0.7939, -5.8127, -1.6934,  1.6705, -3.1824,  0.4215, -4.6470,\n",
      "         -3.5940, -5.7434, -3.8661, -0.8944,  0.5913,  6.2313,  0.2499, -0.8047,\n",
      "          0.6654,  0.5231,  0.0981, -0.3284, -0.2955, -1.5998, -0.8732, -0.2731,\n",
      "          0.1667, -0.5312,  5.4586,  3.7948, -7.3463, -6.7341, -2.3936, -5.1401,\n",
      "         -8.3018, -8.3018, -8.3022, -8.3009, -8.3007, -8.3012, -8.3009, -8.3019,\n",
      "         -8.3012, -8.3028, -8.3010,  2.6640,  6.8100,  3.7307,  4.1508,  6.4514,\n",
      "          1.3471,  2.6609,  6.6534,  2.9637,  0.5070,  5.2483,  5.8856,  2.1856,\n",
      "          1.7527,  1.6096,  6.8810,  3.2181,  1.8677,  1.9588,  6.8039,  0.9778,\n",
      "          1.5032,  6.9033, 10.0286,  1.1927,  1.0272,  6.9201,  8.5111,  1.7383,\n",
      "          6.0809,  7.1547,  0.0941, -0.6406,  3.3031,  0.5503,  0.7487,  0.5732,\n",
      "         -0.7838,  8.9483,  6.5682,  4.9847]])\n",
      "INFO:__name__:next_token: [29995]\n",
      "INFO:__name__:next_token: %\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[ 0.0000, -3.1171, -7.8360, -1.0087, -5.7597, -2.9218, -7.4001, -1.4114,\n",
      "         -0.7412, -0.3361, -0.6186,  0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 29896, 29900, 29900,\n",
      "         29995,    -1]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe president of Turkey is \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/generate.py:168\u001b[0m, in \u001b[0;36mLlama.text_completion\u001b[0;34m(self, prompts, temperature, top_p, max_gen_len, logprobs, echo)\u001b[0m\n\u001b[1;32m    166\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    167\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(x, bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 168\u001b[0m generation_tokens, generation_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    178\u001b[0m         {\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(t),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[1;32m    184\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/generate.py:107\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m    106\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits[:,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m,:\u001b[38;5;241m300\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/model.py:292\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    289\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(mask, diagonal\u001b[38;5;241m=\u001b[39mstart_pos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(h)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 292\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n\u001b[1;32m    295\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(h)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# we convert to 32 bit floats here.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/model.py:215\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    213\u001b[0m h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_norm(x), start_pos, freqs_cis, mask)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# another residual connection!  And again, the norm is applied before the ffn\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/model.py:176\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# This is a gated feed forward network.  W3 acts as a gate on the nonlinear activation\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# from W1.  See: https://arxiv.org/pdf/2002.05202.pdf.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Note that the * represents elementwise multiplication (not matrix multiplication).\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2(F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw3(x))\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/code/my_code/annotated-llama/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l.text_completion([\"The president of Turkey is \"], max_gen_len=5, echo=True, logprobs=True, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3472417-1782-4b7e-aae0-fa3f3fc9f886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
