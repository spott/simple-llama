{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f57f0a-61e7-43a8-96fe-20a65be702a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import basicConfig, INFO, DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8571cc76-74a0-45c8-ad6d-9342f685434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basicConfig(level=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bab1c2-a564-4b0a-be7a-8210fa5ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e7b8f4-1628-4dd9-9af1-db4bb5f311d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reloaded SentencePiece model from ./llama-2-tokenizer/tokenizer.model\n",
      "INFO:root:#words: 32000 - BOS ID: 1 - EOS ID: 2\n",
      "INFO:__name__:model_args: {'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-05, 'vocab_size': 32000, 'max_seq_len': 100, 'max_batch_size': 1, 'ffn_dim_multiplier': None}\n",
      "INFO:__name__:state_dict_map: ['tok_embeddings.weight', 'norm.weight', 'output.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wq.weight', 'layers.28.attention.wk.weight', 'layers.28.attention.wv.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.attention_norm.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wq.weight', 'layers.29.attention.wk.weight', 'layers.29.attention.wv.weight', 'layers.29.attention.wo.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.attention_norm.weight', 'layers.29.ffn_norm.weight', 'layers.30.attention.wq.weight', 'layers.30.attention.wk.weight', 'layers.30.attention.wv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.attention_norm.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.31.attention.wk.weight', 'layers.31.attention.wv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.attention_norm.weight', 'layers.31.ffn_norm.weight', 'rope.freqs']\n",
      "INFO:__name__:unexpected_keys: ['rope.freqs']\n",
      "INFO:__name__:missing_keys: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded in 61.35 seconds\n"
     ]
    }
   ],
   "source": [
    "l = Llama.build(\"./llama-2-7B/\", \"./llama-2-tokenizer/tokenizer.model\", 100, 1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3253b2-4344-4546-9780-aebef5cd70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871,    -1,    -1,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 7, 32000]), tensor([[-1.0587e+00, -1.9833e+00,  3.8633e-01, -9.4345e-01, -2.4158e+00,\n",
      "         -1.2415e+00,  3.5508e-01, -1.6780e+00, -2.4237e+00, -3.4266e+00,\n",
      "         -3.0048e+00, -1.8739e+00,  2.1858e+00,  6.4697e-01,  4.4130e+00,\n",
      "         -2.9997e+00, -1.0571e+00, -2.3853e+00, -1.0119e+00, -1.3353e+00,\n",
      "         -1.0355e+00, -1.2969e+00, -1.7048e+00, -2.4600e+00, -1.3579e+00,\n",
      "         -2.3640e+00, -6.2987e-01,  9.9613e-02, -2.2404e+00, -1.5793e-01,\n",
      "          1.3043e+00, -1.6141e-01,  7.0246e-01, -1.0792e+00, -1.1977e+00,\n",
      "         -1.0584e+00, -1.0579e+00, -1.0577e+00, -1.0566e+00, -1.0567e+00,\n",
      "         -1.0570e+00, -1.0574e+00, -1.0572e+00, -1.0569e+00, -1.0595e+00,\n",
      "         -1.0567e+00, -1.0571e+00, -1.0570e+00, -1.0577e+00, -1.0580e+00,\n",
      "         -1.0582e+00, -1.0577e+00, -1.0571e+00, -1.0593e+00, -1.0572e+00,\n",
      "         -1.0572e+00, -1.0577e+00, -1.0567e+00, -1.0578e+00, -1.0576e+00,\n",
      "         -1.0582e+00, -1.0571e+00, -1.0584e+00, -1.0582e+00, -1.0589e+00,\n",
      "         -1.0582e+00, -1.0592e+00, -1.0572e+00, -1.0587e+00, -1.0578e+00,\n",
      "         -1.0575e+00, -1.0562e+00, -1.0589e+00, -1.0584e+00, -1.0584e+00,\n",
      "         -1.0578e+00, -1.0585e+00, -1.0585e+00, -1.0576e+00, -1.0569e+00,\n",
      "         -1.0581e+00, -1.0580e+00, -1.0569e+00, -1.0587e+00, -1.0572e+00,\n",
      "         -1.0585e+00, -1.0579e+00, -1.0578e+00, -1.0577e+00, -1.0572e+00,\n",
      "         -1.0582e+00, -1.0557e+00, -1.0563e+00, -1.0585e+00, -1.0578e+00,\n",
      "         -1.0587e+00, -1.0569e+00, -1.0582e+00, -1.0579e+00, -1.0583e+00,\n",
      "         -1.0582e+00, -1.0584e+00, -1.0563e+00, -1.0574e+00, -1.0570e+00,\n",
      "         -1.0568e+00, -1.0573e+00, -1.0567e+00, -1.0578e+00, -1.0596e+00,\n",
      "         -1.0577e+00, -1.0580e+00, -1.0579e+00, -1.0583e+00, -1.0583e+00,\n",
      "         -1.0574e+00, -1.0589e+00, -1.0565e+00, -1.0580e+00, -1.0574e+00,\n",
      "         -1.0594e+00, -1.0578e+00, -1.0576e+00, -1.0582e+00, -1.0580e+00,\n",
      "         -1.0586e+00, -1.0565e+00, -1.0581e+00, -1.0589e+00, -1.0575e+00,\n",
      "         -2.5957e+00,  1.3730e+00,  1.5282e+00,  5.7657e-01,  2.1230e+00,\n",
      "          2.7599e+00, -3.5763e-01, -2.5976e+00, -3.8914e+00,  1.2013e+00,\n",
      "         -4.4405e-01, -7.4987e-01,  1.2204e+00,  6.7163e-01,  1.4231e+00,\n",
      "          3.0143e+00,  1.3533e+00,  3.6959e+00,  2.9128e+00, -5.6684e-01,\n",
      "          1.1003e+00,  2.0421e+00,  2.6007e+00,  7.6164e-01,  5.1513e-01,\n",
      "          8.7151e-01, -2.4985e+00,  3.1939e+00,  2.3262e+00,  1.4758e+00,\n",
      "          2.6267e+00,  6.7688e-01, -1.9026e+00,  1.4751e+00,  9.0545e-01,\n",
      "          3.3701e+00, -3.7057e+00, -1.3620e+00, -2.1267e+00,  4.7412e-01,\n",
      "          2.7810e+00, -3.4862e+00, -1.3523e+00, -2.1102e+00,  1.0878e+00,\n",
      "          7.3933e-01,  8.6564e-01, -2.0138e+00, -4.7205e+00,  2.6496e-01,\n",
      "          5.4564e-01, -5.2671e+00, -1.6178e+00, -2.0252e+00,  8.8571e-01,\n",
      "         -3.4203e+00,  8.3820e-01, -4.3428e+00, -3.6443e+00,  1.7394e-01,\n",
      "          1.9967e+00,  1.2984e+00, -1.4090e+00, -9.5700e-01, -4.4830e+00,\n",
      "         -1.0574e+00, -1.0576e+00, -3.9149e+00, -1.0593e+00,  4.5138e+00,\n",
      "         -1.7594e+00,  2.8816e+00,  4.2163e+00,  1.9779e+00, -7.0116e-01,\n",
      "          7.1648e-01, -9.5551e-03, -4.3526e-03,  1.3554e+00, -1.7589e+00,\n",
      "          3.5507e-01, -1.6471e+00,  4.2794e-01, -3.3438e+00,  4.8006e-01,\n",
      "         -4.1037e-01,  2.2107e-01, -2.4766e+00,  1.5425e+00, -9.0345e-01,\n",
      "          2.6042e-01, -3.0319e+00,  1.1756e+00,  7.9327e-01,  1.5892e+00,\n",
      "          4.4268e+00, -2.2179e+00, -3.2731e+00, -1.0195e+00,  6.8333e-01,\n",
      "         -2.2460e+00, -3.3530e+00, -2.4155e+00, -2.8898e+00, -1.8939e+00,\n",
      "         -3.4932e+00, -1.1357e+00, -1.5129e-01, -7.5397e-01, -1.5256e+00,\n",
      "         -2.9938e-01,  2.0194e+00, -1.8552e+00,  2.4359e+00, -8.7751e-01,\n",
      "         -1.5143e+00,  3.5400e+00, -7.5975e-01, -1.0577e+00, -1.0565e+00,\n",
      "         -1.0587e+00, -1.0603e+00, -1.0589e+00, -1.0575e+00, -1.0573e+00,\n",
      "         -1.0570e+00, -1.0578e+00, -1.0574e+00, -1.0586e+00, -5.0086e-01,\n",
      "          2.6327e+00,  1.2236e+00, -1.7408e+00,  1.0371e+00,  1.7425e+00,\n",
      "         -6.1005e-02,  3.0290e+00,  1.2132e+00, -1.6917e-01,  3.7331e-02,\n",
      "          2.4756e+00,  3.2474e-01, -2.1592e-01, -1.2779e+00,  1.2786e-01,\n",
      "         -8.2324e-01,  1.7561e+00, -9.6975e-01,  1.0086e+00,  1.5666e+00,\n",
      "          1.1972e+00,  1.3551e+00,  1.6296e+00, -9.9649e-01, -1.2858e+00,\n",
      "          2.1611e+00,  1.5865e+00,  1.0819e+00,  1.7069e+00,  1.3911e+00,\n",
      "         -1.6606e-01,  1.7926e+00,  1.8928e-01, -1.3283e+00,  2.1105e+00,\n",
      "         -9.0756e-01, -1.6151e+00,  7.1376e-01,  8.8023e-01, -7.3951e-01]])\n",
      "INFO:__name__:probs: torch.Size([1, 32000]), tensor([[3.0105e-08, 6.4475e-09, 3.3467e-07,  ..., 2.1735e-08, 3.0870e-08,\n",
      "         3.5298e-08]])[-1,:300]\n",
      "INFO:__name__:top 10 indexes: tensor([[21955, 16645, 23270,  1617, 12827,  6324,  8916, 11151, 19010,  1780]])\n",
      "INFO:__name__:top 10 temperature probs: tensor([[0.1094, 0.0924, 0.0659, 0.0348, 0.0278, 0.0247, 0.0176, 0.0151, 0.0146,\n",
      "         0.0139]])\n",
      "INFO:__name__:next_token: [21955]\n",
      "INFO:__name__:next_token: spons\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[  0.0000, -10.8764, -11.5281,  -8.8219, -14.1848, -11.5533, -11.0348,\n",
      "          -4.1437,   0.0000,   0.0000,   0.0000,   0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 21955,    -1,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-0.4786, -1.1786, -1.0692, -1.7315, -1.6878, -1.7085,  0.8791, -2.2036,\n",
      "         -1.0290, -2.3405, -1.4507, -1.0386,  2.4753,  0.2615,  3.9253, -3.4826,\n",
      "         -0.4782, -1.5952,  0.8157, -0.5119,  0.1917, -1.8446,  0.4841, -1.9255,\n",
      "         -0.4386, -0.9635,  0.1273,  0.8339, -0.7695,  1.7150,  2.7147, -0.6744,\n",
      "          1.1512, -0.1665, -0.2729, -0.4781, -0.4781, -0.4775, -0.4772, -0.4775,\n",
      "         -0.4774, -0.4777, -0.4769, -0.4772, -0.4794, -0.4769, -0.4776, -0.4778,\n",
      "         -0.4781, -0.4791, -0.4785, -0.4789, -0.4776, -0.4790, -0.4774, -0.4779,\n",
      "         -0.4785, -0.4780, -0.4783, -0.4778, -0.4783, -0.4779, -0.4794, -0.4780,\n",
      "         -0.4790, -0.4783, -0.4796, -0.4775, -0.4791, -0.4785, -0.4787, -0.4775,\n",
      "         -0.4793, -0.4784, -0.4784, -0.4789, -0.4789, -0.4783, -0.4788, -0.4771,\n",
      "         -0.4787, -0.4791, -0.4767, -0.4791, -0.4777, -0.4790, -0.4779, -0.4787,\n",
      "         -0.4782, -0.4776, -0.4788, -0.4769, -0.4775, -0.4791, -0.4786, -0.4792,\n",
      "         -0.4769, -0.4788, -0.4783, -0.4788, -0.4783, -0.4776, -0.4769, -0.4774,\n",
      "         -0.4784, -0.4776, -0.4785, -0.4775, -0.4785, -0.4796, -0.4780, -0.4783,\n",
      "         -0.4788, -0.4780, -0.4786, -0.4773, -0.4790, -0.4771, -0.4790, -0.4780,\n",
      "         -0.4791, -0.4776, -0.4770, -0.4786, -0.4784, -0.4798, -0.4773, -0.4793,\n",
      "         -0.4795, -0.4771, -2.2525,  0.5690,  1.7366,  2.0527,  2.2948,  3.8059,\n",
      "         -2.2865, -2.6857, -0.2710,  0.0196,  3.4251,  0.4290, -0.6425,  0.3350,\n",
      "          2.2033,  1.5169,  3.8287,  4.6925,  2.5117, -0.7536,  1.4542,  2.4843,\n",
      "          3.6564, -0.3722, -0.0871,  1.6572, -1.0229,  1.7197,  1.0206,  1.2549,\n",
      "          1.8276, -0.0366, -3.3682,  2.2199,  1.8757,  0.9704, -2.6834, -1.4888,\n",
      "         -2.3546,  1.1853,  1.9523, -2.0457, -0.3592, -0.5173,  2.6882,  1.2347,\n",
      "          2.0029, -2.6241, -2.2277,  2.1152,  2.9379, -2.5103, -1.7209, -0.8321,\n",
      "         -0.0402, -3.4103,  3.8049, -1.3480, -2.4540,  0.5557,  2.2576,  1.3968,\n",
      "          0.3923, -0.7024, -4.9718, -0.4772, -0.4776, -2.3567, -0.4795,  5.8892,\n",
      "          1.2773,  3.4654,  3.5781,  2.7619, -1.6206,  0.9333,  0.7116,  1.5334,\n",
      "          3.4311, -1.5664,  0.1407,  1.3938,  3.6566, -1.7779, -0.3943,  1.2334,\n",
      "          0.2879, -1.7438,  2.1609,  0.3212, -0.1770, -1.8314,  2.4921,  1.2179,\n",
      "          3.4813,  4.8156, -1.3029, -1.1937,  0.5351,  0.8373, -1.0207, -2.8092,\n",
      "         -0.9825, -2.8731, -1.2951, -2.0170, -0.9546, -1.2372, -0.9933, -0.7077,\n",
      "         -0.7784,  0.9303, -1.7369,  2.2800, -0.9766, -0.8182,  1.9711, -0.2664,\n",
      "         -0.4793, -0.4766, -0.4789, -0.4805, -0.4788, -0.4783, -0.4779, -0.4784,\n",
      "         -0.4773, -0.4774, -0.4783, -0.6957,  1.4496,  0.9154, -2.4691,  0.4371,\n",
      "          1.6339, -0.0065,  2.4229, -0.2602, -1.1047,  0.5007,  1.6818, -0.2010,\n",
      "          0.8612, -2.0831,  0.1140, -1.0106,  0.9430, -1.1522,  0.4863,  0.1050,\n",
      "          1.5001,  1.9509,  0.9185, -1.0739, -1.3815,  1.8854,  0.2602,  0.3644,\n",
      "          0.0946,  0.7459, -1.2516,  1.9025,  0.7044, -1.9228,  1.3513,  0.6028,\n",
      "         -2.0733, -0.2562, -1.1077, -1.8170]])\n",
      "INFO:__name__:probs: torch.Size([1, 32000]), tensor([[6.4729e-08, 2.0159e-08, 2.4191e-08,  ..., 1.9543e-09, 1.7225e-09,\n",
      "         2.6595e-06]])[-1,:300]\n",
      "INFO:__name__:top 10 indexes: tensor([[25714,  6693, 25482, 19010, 19378, 21955, 13961, 20585,  5991,  4516]])\n",
      "INFO:__name__:top 10 temperature probs: tensor([[0.0647, 0.0371, 0.0347, 0.0303, 0.0295, 0.0292, 0.0226, 0.0226, 0.0150,\n",
      "         0.0149]])\n",
      "INFO:__name__:next_token: [25714]\n",
      "INFO:__name__:next_token: heits\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[  0.0000, -10.8764, -11.5281,  -8.8219, -14.1848, -11.5533, -11.0348,\n",
      "          -4.1437,  -4.5458,   0.0000,   0.0000,   0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 21955, 25714,    -1,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-1.5202e+00, -2.9356e+00,  7.6037e-01, -3.7793e-01, -2.5295e+00,\n",
      "         -2.5795e+00,  1.0344e-01, -3.1062e+00, -2.8728e+00, -2.9583e+00,\n",
      "         -1.1839e+00, -1.2500e+00,  1.6609e+00,  1.2321e+00,  2.8684e+00,\n",
      "         -2.5585e+00, -1.5188e+00, -2.9125e+00, -1.2828e+00, -2.1435e+00,\n",
      "         -1.3306e+00, -3.7629e+00, -1.1510e+00, -3.1608e+00, -9.2991e-01,\n",
      "         -3.8850e+00, -6.5510e-01,  3.5703e-02, -1.2324e+00,  3.4486e+00,\n",
      "          2.8263e+00, -3.9299e-01,  2.3683e+00, -1.1797e-01, -1.1066e+00,\n",
      "         -1.5199e+00, -1.5198e+00, -1.5190e+00, -1.5194e+00, -1.5187e+00,\n",
      "         -1.5183e+00, -1.5188e+00, -1.5183e+00, -1.5191e+00, -1.5217e+00,\n",
      "         -1.5185e+00, -1.5184e+00, -1.5181e+00, -1.5208e+00, -1.5198e+00,\n",
      "         -1.5197e+00, -1.5202e+00, -1.5191e+00, -1.5208e+00, -1.5189e+00,\n",
      "         -1.5197e+00, -1.5196e+00, -1.5191e+00, -1.5199e+00, -1.5197e+00,\n",
      "         -1.5200e+00, -1.5189e+00, -1.5208e+00, -1.5197e+00, -1.5204e+00,\n",
      "         -1.5200e+00, -1.5208e+00, -1.5200e+00, -1.5201e+00, -1.5189e+00,\n",
      "         -1.5196e+00, -1.5186e+00, -1.5203e+00, -1.5207e+00, -1.5208e+00,\n",
      "         -1.5196e+00, -1.5202e+00, -1.5184e+00, -1.5195e+00, -1.5174e+00,\n",
      "         -1.5187e+00, -1.5177e+00, -1.5186e+00, -1.5211e+00, -1.5192e+00,\n",
      "         -1.5203e+00, -1.5192e+00, -1.5196e+00, -1.5185e+00, -1.5187e+00,\n",
      "         -1.5204e+00, -1.5174e+00, -1.5176e+00, -1.5201e+00, -1.5195e+00,\n",
      "         -1.5210e+00, -1.5176e+00, -1.5191e+00, -1.5188e+00, -1.5195e+00,\n",
      "         -1.5188e+00, -1.5180e+00, -1.5187e+00, -1.5179e+00, -1.5201e+00,\n",
      "         -1.5186e+00, -1.5192e+00, -1.5188e+00, -1.5194e+00, -1.5210e+00,\n",
      "         -1.5190e+00, -1.5196e+00, -1.5199e+00, -1.5200e+00, -1.5202e+00,\n",
      "         -1.5185e+00, -1.5195e+00, -1.5180e+00, -1.5207e+00, -1.5197e+00,\n",
      "         -1.5202e+00, -1.5186e+00, -1.5187e+00, -1.5206e+00, -1.5198e+00,\n",
      "         -1.5206e+00, -1.5185e+00, -1.5201e+00, -1.5204e+00, -1.5186e+00,\n",
      "         -2.7893e+00,  9.3812e-01,  1.9334e+00,  2.9386e+00, -2.1840e-01,\n",
      "          6.4756e+00, -2.8708e+00, -4.3421e+00, -2.8260e+00,  1.1198e+00,\n",
      "          3.1575e-01, -1.2395e+00,  1.9809e+00, -7.0398e-01,  3.9691e+00,\n",
      "          1.1171e+00,  3.2311e+00,  4.9306e+00,  2.0050e+00, -1.9727e+00,\n",
      "          2.6398e+00,  5.9339e-01,  6.4707e-01, -5.5444e-01, -6.1976e-01,\n",
      "          1.8330e+00, -4.0198e+00,  1.3873e-01,  5.4241e-01,  1.7636e+00,\n",
      "          1.1060e+00, -7.1207e-01, -1.4773e+00,  2.5645e+00,  2.2340e+00,\n",
      "          5.6971e-01, -3.2201e+00,  8.6766e-01, -2.4056e+00,  1.8479e+00,\n",
      "          2.1645e+00, -2.6850e-01, -1.5835e-02, -1.0947e+00,  1.4608e+00,\n",
      "          5.1860e-01,  3.0540e+00, -2.7344e+00, -3.1945e+00,  6.6166e-01,\n",
      "          3.0550e+00, -1.3436e+00, -3.7423e+00, -1.1104e+00,  2.7269e+00,\n",
      "         -3.3360e+00, -4.2386e-01, -1.2870e+00, -3.1541e+00, -1.3302e+00,\n",
      "          2.0265e+00,  1.7085e+00,  1.0467e+00,  4.5961e-01, -4.2609e+00,\n",
      "         -1.5189e+00, -1.5194e+00, -1.5342e+00, -1.5205e+00,  6.2710e+00,\n",
      "         -2.3302e+00,  2.2601e+00,  3.0572e+00,  1.5393e+00, -1.0735e+00,\n",
      "          5.3188e-01, -1.5453e+00,  2.2452e+00,  2.8558e+00, -4.4371e-01,\n",
      "          5.5624e-03, -6.1299e-01,  1.1173e+00, -2.7104e+00, -6.4279e-01,\n",
      "          5.5409e-01,  9.5072e-01,  4.4620e-01,  2.5612e+00,  2.7772e-01,\n",
      "          1.8925e+00, -1.1417e+00,  1.4378e+00, -7.0894e-01,  1.6541e+00,\n",
      "          2.3810e+00, -9.0849e-01,  9.8727e-03,  9.6099e-02,  1.3673e+00,\n",
      "         -1.5709e+00,  7.7363e-02, -6.2596e-01, -6.5105e-01,  9.4951e-02,\n",
      "         -9.7916e-01, -1.0714e-02, -1.3186e+00, -3.8916e-01, -1.4017e+00,\n",
      "         -1.0220e-01,  2.0849e+00, -2.0003e-01,  1.1380e+00, -1.7263e+00,\n",
      "         -2.1291e+00,  1.8523e+00, -1.1385e+00, -1.5187e+00, -1.5181e+00,\n",
      "         -1.5196e+00, -1.5216e+00, -1.5200e+00, -1.5196e+00, -1.5191e+00,\n",
      "         -1.5193e+00, -1.5182e+00, -1.5182e+00, -1.5198e+00,  7.4282e-01,\n",
      "          1.9257e+00,  1.6132e+00, -2.5409e+00,  3.3381e-01,  1.7176e+00,\n",
      "         -3.0502e-01,  3.2525e-01,  8.3122e-01, -7.0366e-01,  8.8177e-01,\n",
      "          2.2678e+00, -1.2092e+00, -6.5891e-02, -2.9687e+00,  1.0183e+00,\n",
      "         -1.6028e+00,  2.0344e+00, -1.6725e+00,  8.9827e-01, -8.5725e-01,\n",
      "          1.0264e+00,  2.3482e+00,  1.6173e+00, -1.7429e+00, -6.9152e-02,\n",
      "          2.8232e+00,  6.5042e-01,  8.6732e-01,  1.8529e+00,  1.8161e+00,\n",
      "         -9.6560e-01, -7.8222e-01,  2.9675e-01, -1.8672e+00,  2.5219e-01,\n",
      "          6.0177e-01, -1.3063e+00,  5.0537e-01,  3.2889e-01, -1.1163e+00]])\n",
      "INFO:__name__:probs: torch.Size([1, 32000]), tensor([[2.2465e-08, 2.1233e-09, 1.0052e-06,  ..., 5.7369e-08, 1.5700e-09,\n",
      "         4.0762e-08]])[-1,:300]\n",
      "INFO:__name__:top 10 indexes: tensor([[  866, 15450, 27301, 15533,  2306, 11222, 26379, 21787,  4481,  8590]])\n",
      "INFO:__name__:top 10 temperature probs: tensor([[0.0959, 0.0480, 0.0414, 0.0285, 0.0281, 0.0253, 0.0243, 0.0231, 0.0227,\n",
      "         0.0181]])\n",
      "INFO:__name__:next_token: [7501]\n",
      "INFO:__name__:next_token: dig\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[  0.0000, -10.8764, -11.5281,  -8.8219, -14.1848, -11.5533, -11.0348,\n",
      "          -4.1437,  -4.5458,  -7.0826,   0.0000,   0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 21955, 25714,  7501,\n",
      "            -1,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-3.1832, -6.8641,  1.0889, -0.5541, -1.5449, -1.9531, -0.5917, -1.7239,\n",
      "         -1.8195, -2.2641, -2.3629, -0.5075,  2.4583,  3.3695,  2.8027, -1.9588,\n",
      "         -3.1814, -2.6733, -1.5203, -0.9950, -1.4251, -2.0455, -1.3147, -3.0162,\n",
      "         -2.6675, -4.3189, -1.6175, -1.6331, -3.4734, -0.8410,  0.2992, -0.5624,\n",
      "          1.6304, -1.4989, -1.2777, -3.1828, -3.1823, -3.1829, -3.1823, -3.1822,\n",
      "         -3.1817, -3.1822, -3.1819, -3.1813, -3.1833, -3.1819, -3.1823, -3.1823,\n",
      "         -3.1831, -3.1833, -3.1842, -3.1837, -3.1822, -3.1843, -3.1825, -3.1830,\n",
      "         -3.1822, -3.1821, -3.1831, -3.1823, -3.1836, -3.1819, -3.1839, -3.1835,\n",
      "         -3.1833, -3.1837, -3.1840, -3.1827, -3.1833, -3.1833, -3.1833, -3.1813,\n",
      "         -3.1841, -3.1834, -3.1833, -3.1827, -3.1838, -3.1828, -3.1825, -3.1817,\n",
      "         -3.1836, -3.1821, -3.1818, -3.1833, -3.1826, -3.1838, -3.1828, -3.1823,\n",
      "         -3.1828, -3.1823, -3.1838, -3.1810, -3.1817, -3.1842, -3.1831, -3.1839,\n",
      "         -3.1816, -3.1834, -3.1828, -3.1841, -3.1826, -3.1826, -3.1821, -3.1818,\n",
      "         -3.1832, -3.1815, -3.1825, -3.1821, -3.1828, -3.1844, -3.1819, -3.1833,\n",
      "         -3.1829, -3.1829, -3.1846, -3.1821, -3.1842, -3.1812, -3.1836, -3.1823,\n",
      "         -3.1837, -3.1829, -3.1827, -3.1842, -3.1837, -3.1841, -3.1819, -3.1837,\n",
      "         -3.1841, -3.1817, -2.4273,  1.4925,  1.7063,  2.3845,  0.2004,  1.0080,\n",
      "         -1.1728, -1.9346, -5.1454, -0.0288,  1.7726, -0.8614,  0.8343, -2.8451,\n",
      "          0.9995,  2.6211,  3.3936,  1.8288,  2.4470, -0.3085,  1.3862,  3.5125,\n",
      "          0.9776, -0.7679, -1.8340, -1.8803, -4.9799,  1.6918,  0.8478,  1.8885,\n",
      "          1.4821,  0.6208, -3.6004,  1.6594, -0.6068,  0.8781, -3.1936,  0.1667,\n",
      "         -3.5107,  0.0488,  2.2073, -2.7547, -0.3077, -1.4874,  0.9483,  0.2227,\n",
      "          0.9473, -2.1785, -1.4925, -0.9580,  0.1601, -3.2201, -1.5356, -1.6188,\n",
      "         -0.4847, -3.9262, -0.2483, -3.3056, -4.0765, -3.7554, -1.1379, -0.9238,\n",
      "         -3.4887, -0.8273, -4.7247, -3.1826, -3.1816, -2.0463, -3.1847,  2.9048,\n",
      "         -1.1383,  3.7444,  1.6509, -0.4466, -1.0262, -1.1768, -2.1181, -0.3013,\n",
      "          1.5115, -1.1598, -1.0194,  0.1654,  0.4674, -3.5790, -0.4082, -2.1898,\n",
      "         -1.5155, -1.4257,  2.0473, -0.1181,  1.8509, -4.6528, -0.2931, -3.2087,\n",
      "         -2.0988,  1.7642, -4.4579, -0.4801, -1.1372, -0.8607, -3.1377, -0.4641,\n",
      "         -0.3284, -2.5879, -0.4640, -0.9264, -1.2774, -1.6361, -1.7043,  0.2835,\n",
      "         -0.0169, -0.2120, -3.1416,  2.0080, -2.9823, -2.6933,  2.7628, -1.7254,\n",
      "         -3.1833, -3.1822, -3.1830, -3.1851, -3.1834, -3.1820, -3.1827, -3.1824,\n",
      "         -3.1820, -3.1830, -3.1834, -0.0771,  2.5388,  3.3437, -0.7597,  1.2226,\n",
      "          2.4383,  2.1364,  2.3296,  3.7242, -0.6547,  1.0408,  2.8278,  2.0919,\n",
      "          1.2925, -0.1014,  2.2595,  0.6766,  3.0617,  0.8655,  1.4424,  1.9932,\n",
      "          2.3979,  2.9934,  2.7113, -0.7937,  0.0409,  3.0178,  1.2856,  1.2486,\n",
      "          2.1926,  1.5151,  0.4865,  0.5373,  1.6418,  1.0360,  3.0515,  0.9271,\n",
      "         -1.6419,  2.0359,  1.4232, -0.6908]])\n",
      "INFO:__name__:probs: torch.Size([1, 32000]), tensor([[2.5249e-09, 5.4688e-12, 3.1222e-06,  ..., 2.1181e-09, 2.3356e-08,\n",
      "         4.1301e-08]])[-1,:300]\n",
      "INFO:__name__:top 10 indexes: tensor([[ 9167, 25850,  9815, 19793, 10699,  1267, 26690,  9789,   640, 11222]])\n",
      "INFO:__name__:top 10 temperature probs: tensor([[0.0847, 0.0418, 0.0364, 0.0350, 0.0337, 0.0287, 0.0250, 0.0189, 0.0137,\n",
      "         0.0130]])\n",
      "INFO:__name__:next_token: [10270]\n",
      "INFO:__name__:next_token: ifs\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[  0.0000, -10.8764, -11.5281,  -8.8219, -14.1848, -11.5533, -11.0348,\n",
      "          -4.1437,  -4.5458,  -7.0826,  -6.2562,   0.0000]])\n",
      "INFO:__name__:tokens: torch.Size([1, 12]), tensor([[    1,   450,  6673,   310, 26459,   338, 29871, 21955, 25714,  7501,\n",
      "         10270,    -1]])\n",
      "INFO:__name__:logits: torch.Size([1, 1, 32000]), tensor([[-1.1920, -1.4502,  0.8593, -0.9160, -0.7229, -1.4505, -0.9427, -2.1642,\n",
      "         -2.0854, -3.0104, -0.9365, -0.4514,  1.7337,  1.4942,  3.3344, -2.1525,\n",
      "         -1.1897, -3.5880, -0.5395, -0.8388, -1.9297, -1.9228, -1.1855, -3.7461,\n",
      "         -1.6472, -3.1132, -0.7662, -0.1127, -1.7046,  2.6830,  1.7454, -0.9624,\n",
      "          1.4609, -0.8142, -0.8082, -1.1916, -1.1915, -1.1911, -1.1900, -1.1906,\n",
      "         -1.1896, -1.1903, -1.1905, -1.1902, -1.1921, -1.1901, -1.1904, -1.1908,\n",
      "         -1.1912, -1.1914, -1.1922, -1.1916, -1.1902, -1.1925, -1.1905, -1.1913,\n",
      "         -1.1908, -1.1910, -1.1904, -1.1913, -1.1912, -1.1905, -1.1912, -1.1912,\n",
      "         -1.1916, -1.1918, -1.1918, -1.1904, -1.1916, -1.1913, -1.1906, -1.1891,\n",
      "         -1.1923, -1.1911, -1.1924, -1.1905, -1.1912, -1.1907, -1.1911, -1.1903,\n",
      "         -1.1912, -1.1907, -1.1897, -1.1916, -1.1901, -1.1920, -1.1914, -1.1908,\n",
      "         -1.1908, -1.1909, -1.1914, -1.1894, -1.1899, -1.1918, -1.1909, -1.1917,\n",
      "         -1.1901, -1.1904, -1.1912, -1.1912, -1.1915, -1.1907, -1.1889, -1.1908,\n",
      "         -1.1907, -1.1897, -1.1912, -1.1901, -1.1894, -1.1925, -1.1906, -1.1908,\n",
      "         -1.1918, -1.1916, -1.1923, -1.1902, -1.1921, -1.1894, -1.1918, -1.1907,\n",
      "         -1.1919, -1.1909, -1.1906, -1.1916, -1.1918, -1.1919, -1.1899, -1.1915,\n",
      "         -1.1918, -1.1898, -3.4796,  0.1079,  0.6063,  1.9859,  0.6733,  4.4386,\n",
      "         -0.6932,  0.0779, -2.6835,  0.5694, -1.0001, -0.9855,  2.2113,  1.0353,\n",
      "          2.9656,  1.9928,  2.7554,  3.1787,  1.2360, -1.0533,  1.6109,  1.8269,\n",
      "          1.9652,  0.6129, -1.4404, -0.2264, -4.5839,  1.2016, -0.2123,  2.1368,\n",
      "          0.9500, -0.3503, -2.7883,  2.4625,  1.0477,  0.3116, -3.1972, -0.8879,\n",
      "         -3.4454,  0.8619,  0.3085, -2.0787, -1.0038, -1.2483,  1.6822,  0.1555,\n",
      "          3.1719, -0.4405, -2.3980,  1.5238,  2.3916, -2.1726, -3.3370, -0.5138,\n",
      "          1.1846, -1.3307,  0.5219, -2.8081, -1.6336, -0.3289,  2.9804, -1.5780,\n",
      "         -0.5752, -1.0959, -5.6173, -1.1899, -1.1900, -3.5366, -1.1927,  6.2864,\n",
      "          0.6675,  2.8593,  3.5513,  1.3514,  0.7773,  0.2128, -2.3022, -0.7107,\n",
      "          2.1431, -0.8848,  0.2859,  1.2411,  1.0819, -1.1076,  1.6542,  0.1090,\n",
      "         -0.7804, -2.0087,  2.4153,  1.9998,  0.5672, -0.7974,  3.2034, -0.5502,\n",
      "         -0.0505,  2.2045, -0.5616, -0.7780, -0.9118,  0.5095, -2.3493, -1.8913,\n",
      "         -1.7074, -1.9919, -1.6342, -2.0385, -1.4496, -1.0847, -0.8025, -0.0374,\n",
      "         -0.9849,  0.9380, -1.1664,  1.9589, -1.7559, -1.6652,  1.7654, -1.8376,\n",
      "         -1.1906, -1.1893, -1.1919, -1.1927, -1.1921, -1.1903, -1.1910, -1.1907,\n",
      "         -1.1901, -1.1906, -1.1906,  0.6541,  2.4973,  1.0871, -0.4654,  0.5371,\n",
      "          1.6488,  0.5302, -0.0387,  0.4581, -0.9147,  0.2059,  3.0685,  1.3039,\n",
      "          0.1461, -1.0507,  0.7243, -1.2504,  1.5725, -0.8275,  1.1587,  2.0131,\n",
      "         -0.0429,  1.8021,  1.7921, -0.9430,  0.4943,  2.6658,  0.9673,  1.3428,\n",
      "          1.1045,  1.9045, -0.2520,  5.0187,  0.1518,  0.4908,  1.3383, -1.2440,\n",
      "         -0.0454,  1.2699,  1.5420, -0.0741]])\n",
      "INFO:__name__:probs: torch.Size([1, 32000]), tensor([[2.9707e-08, 1.9318e-08, 9.0698e-07,  ..., 1.4386e-09, 1.7096e-08,\n",
      "         6.8216e-08]])[-1,:300]\n",
      "INFO:__name__:top 10 indexes: tensor([[10538, 21787, 17787,  9815,  8916, 21955, 16101, 19010, 26690, 24301]])\n",
      "INFO:__name__:top 10 temperature probs: tensor([[0.0819, 0.0660, 0.0360, 0.0289, 0.0281, 0.0281, 0.0265, 0.0244, 0.0129,\n",
      "         0.0122]])\n",
      "INFO:__name__:next_token: [17787]\n",
      "INFO:__name__:next_token: Fou\n",
      "INFO:__name__:logprobs: torch.Size([1, 12]), tensor([[  0.0000, -10.8764, -11.5281,  -8.8219, -14.1848, -11.5533, -11.0348,\n",
      "          -4.1437,  -4.5458,  -7.0826,  -6.2562,  -4.8809]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generation': 'The president of Turkey is  sponsheitsdigifs Fou',\n",
       "  'tokens': ['',\n",
       "   'The',\n",
       "   'president',\n",
       "   'of',\n",
       "   'Turkey',\n",
       "   'is',\n",
       "   '',\n",
       "   'spons',\n",
       "   'heits',\n",
       "   'dig',\n",
       "   'ifs',\n",
       "   'Fou'],\n",
       "  'logprobs': [0.0,\n",
       "   -10.876371383666992,\n",
       "   -11.528057098388672,\n",
       "   -8.821935653686523,\n",
       "   -14.184791564941406,\n",
       "   -11.553332328796387,\n",
       "   -11.034751892089844,\n",
       "   -4.143719673156738,\n",
       "   -4.545821189880371,\n",
       "   -7.082568645477295,\n",
       "   -6.256226062774658,\n",
       "   -4.8809404373168945]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.text_completion([\"The president of Turkey is \"], max_gen_len=5, echo=True, logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3472417-1782-4b7e-aae0-fa3f3fc9f886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
